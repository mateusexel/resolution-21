{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import spacy.cli\n",
    "# spacy.cli.download(\"pt_core_news_sm\")\n",
    "# nlp= spacy.load('pt_core_news_sm')\n",
    "\n",
    "# my_file = open(\"stopwords.txt\", \"r\")\n",
    "# stopwords = [x.strip(' ') for x in my_file.read().split('\\n')]\n",
    "dfthousand = pd.read_json('../data/portal_merge_eco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfthousand['NM_ARQ']=[x.split('.')[0]+'.txt' for x in dfthousand['NM_ARQ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def open_txt_usb(file):\n",
    "#     print(file)\n",
    "    try:\n",
    "        with open(f'/media/mateus/disk/fee/txts/{file}','r') as file:\n",
    "            text = file.read()\n",
    "            return text\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important match fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200423174612UPb10f70b50b5c4933afce95c2fb6f9a04.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfthousand['NM_ARQ'][36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfthousand['fulltext']=[open_txt_usb(z) for z in dfthousand['NM_ARQ']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing white spaces, stop words and concating number to percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner_and_stopremover(text):\n",
    "#     text=''.join(text)\n",
    "\n",
    "    ctext=text.lower()\n",
    "#     ctext=re.sub(r'\\n', r' ',ctext)\n",
    "    ctext=re.sub(r'(\\d)\\s+(%)', r'\\1\\2',ctext)\n",
    "    ctext=' '.join(ctext.split())\n",
    "\n",
    "    nonstop=[]    \n",
    "    for word in ctext.split():\n",
    "        if word not in stopwords:\n",
    "            nonstop.append(word)\n",
    "            \n",
    "#     return ctext\n",
    "    return ' '.join(nonstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfthousand=pd.read_json('./dfthousand_with_clean_text.json')\n",
    "dfthousand['cleanfulltext']=[cleaner_and_stopremover(z) for z in dfthousand['fulltext']]\n",
    "# dfthousand.to_json('./dfthousand_with_clean_text.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfthousand['cleanfulltext'], np.array(pd.get_dummies(dfthousand['fee'])), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 71)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got Series)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7f11248436f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mx_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0my_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7f11248436f1>\u001b[0m in \u001b[0;36mx_2_vec\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mx_2_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwordidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             )\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_docs_and_golds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got Series)"
     ]
    }
   ],
   "source": [
    "\n",
    "def x_2_vec(x):\n",
    "    x_vec=np.zeros((24307,96))\n",
    "    for wordidx, word in enumerate(nlp(x)):\n",
    "        x_vec[wordidx]=word.vector\n",
    "    \n",
    "x_2_vec(X_test)\n",
    "\n",
    "def y_2_vec(y):\n",
    "    \n",
    "    nlp(y).vector\n",
    "\n",
    "# y_train2vec=np.zeros((416,96))\n",
    "# for fileidx, file in enumerate(list(y_train.replace('-','n√£o'))):\n",
    "    \n",
    "#     for wordidx ,word in enumerate(nlp(file)):\n",
    "#         y_train2vec[fileidx]=word.vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2vec=np.zeros((416,24307,96))\n",
    "\n",
    "for fileidx, file in enumerate(X_train):\n",
    "#     print(file)\n",
    "#     temp_np=np.zeros((len(file.split()),300))\n",
    "#     print(len(file.split()))\n",
    "    for wordidx ,word in enumerate(nlp(file)):\n",
    "#         print(word)\n",
    "# #         print(wordidx)\n",
    "        X_train2vec[fileidx][wordidx]=word.vector\n",
    "#     print(temp_np)\n",
    "#     print('------------------------')\n",
    "# #         print(j.vector.shape)\n",
    "np.save('./np_vecs/spacy/X_train', X_train2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2vec=np.zeros((205,24307,96))\n",
    "\n",
    "for fileidx, file in enumerate(X_test):\n",
    "#     print(file)\n",
    "#     temp_np=np.zeros((len(file.split()),300))\n",
    "#     print(len(file.split()))\n",
    "    for wordidx ,word in enumerate(nlp(file)):\n",
    "#         print(word)\n",
    "# #         print(wordidx)\n",
    "        X_test2vec[fileidx][wordidx]=word.vector\n",
    "#     print(temp_np)\n",
    "#     print('------------------------')\n",
    "# #         print(j.vector.shape)\n",
    "np.save('./np_vecs/spacy/X_test', X_test2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train2vec=np.zeros((416,96))\n",
    "# for fileidx, file in enumerate(list(y_train.replace('-','n√£o'))):\n",
    "    \n",
    "#     for wordidx ,word in enumerate(nlp(file)):\n",
    "#         y_train2vec[fileidx]=word.vector\n",
    "        \n",
    "np.save('./np_vecs/spacy/y_train', y_train)\n",
    "    \n",
    "    \n",
    "# y_test2vec=np.zeros((205,96))\n",
    "# for fileidx, file in enumerate(list(y_test.replace('-','n√£o'))):\n",
    "    \n",
    "#     for wordidx ,word in enumerate(nlp(file)):\n",
    "#         y_test2vec[fileidx]=word.vector\n",
    "        \n",
    "np.save('./np_vecs/spacy/y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa=np.array([1,2,30])\n",
    "ya=np.array([1,2,30])\n",
    "np.random.seed(42) \n",
    "np.random.shuffle(xa)\n",
    "np.random.shuffle(ya)\n",
    "print(xa,ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame({'X':cleanfulltext2vec, 'Y':y2vec})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cleanfulltext2vec, y2vec, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./np_vecs/spacy/X_train', X_train)\n",
    "np.save('./np_vecs/spacy/X_test', X_test)\n",
    "np.save('./np_vecs/spacy/y_train', y_train)\n",
    "np.save('./np_vecs/spacy/y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./np_vecs/spacy/cleanfulltext2vec', cleanfulltext2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
